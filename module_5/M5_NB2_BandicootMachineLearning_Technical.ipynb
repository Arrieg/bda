{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">Python 2.7 Jupyter Notebook</div>\n",
    "\n",
    "# Predictive Modeling Using Bandicoot Features\n",
    "<br>\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Note that this notebook contains advanced exercises applicable only to students who wish to deepen their understanding and qualify for bonus marks as part of the technical track.</b> You will be able to achieve 100% for this notebook by only completing exercises 1 to 4. An optional advanced exercise can be completed to qualify for bonus marks.\n",
    "</div>\n",
    "\n",
    "### Your completion of the notebook exercises will be graded based on your ability to: \n",
    "\n",
    "> **Understand**: Does your pseudo-code and/or comments show evidence that you recall and understand technical concepts?\n",
    "\n",
    "> **Apply**: Are you able to execute code, using the supplied examples, that perform the required functionality on supplied or generated data sets? \n",
    "\n",
    "> **Analyze**: Are you able to pick the relevant method, library or resolve specific stated questions?\n",
    "\n",
    "> **Evaluate**: Are you able to interpret the results and justify your interpretation based on the observed data?\n",
    "\n",
    "> **Create**: Your ability to produce notebooks that serve as computational record of a session that can be used to share your insights with others? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook introduction\n",
    "This notebook serve as a brief introduction to machine learning. It serves as an introduction to this exciting field and provides you with hands on examples of the various steps you are likely to perform. We utilize a number of the Python libraries and methods introduced in earlier modules to generate features and compute behavioral indicators.\n",
    "\n",
    "> **Note**:\n",
    "\n",
    "> Many of to terms introduced will be new to many of the students, but will make more sense when you reach the end of the notebook.\n",
    "\n",
    "The approach followed in this notebook entails computing a number of behavioral indicators, defining the target variable before going back to steps introduced in previous modules such as performing exploratory data analysis and preprocessing of data. Classification with cross validation is followed by a section on the the receiver operation characteristic (ROC) curve. In the final section we touch on optimal classifier parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Machine Learning in Python\n",
    "In the previous module we evaluated more 1400 behavioral indicators for many users in the Friends and Family dataset. The next logical questions to ask is what are these indicators good for?  How can you process or make sense of so many variables or features? The short answer is without the help of computers it is virtually impossible. Thanks to **machine learning** we can extract answers to meaningful questions using such vast amounts of data. Examples of such questions that can be of interest include  the following; \n",
    "- What is a person's gender?  \n",
    "- How susceptible to marketing are they? \n",
    "- Are the patterns in data suggesting a higher probability that the person will not be making use of our service? \n",
    "- What is the propensity of them taking up a new product or service offering? \n",
    "- Who is best positioned to assist them should they engage with our organization through any one of our channels? \n",
    "- How does their usage patterns inform product design and development?\n",
    "- Who in a community are most at risk to an epidemic or disease outbreak?\n",
    "\n",
    "\n",
    "As humans amass huge volumes of data, machine learning is increasingly become central in being able to infer and make prediction from this data. \n",
    "\n",
    "### What is Machine Learning?\n",
    "\n",
    "According to the [SAS institute](http://www.sas.com/en_us/insights/analytics/machine-learning.html), *machine learning is a method of data analysis that automates analytical model building. Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed where to look*. There are two main classes of machine learning algorithms: (i) supervised and (ii) unsupervised learning. \n",
    "\n",
    "In unsupervised learning, the objective is to identify or infer a function to describe hidden structure or similarity of patterns in unlabeled data. In module 4, we were introduced to three different methods that can be used for clustering and community detection of graph networks, namely hierarchical clustering, modularity maximization, and spectral graph partitioning. These methods belong to the unsupervised learning family of machine learning algorithms.\n",
    "\n",
    "In supervised learning, which is the focus of this notebook, not only is one provided with a set of features ( $ {X}_i$, for $i=1,...,N$) but also a set of labels ${y}_i, i=1,...,N$, where each $y_i$ corresponds to $X_i$. To give a concrete example, we have already been introduced to the very rich set of features one can derive from mobile phone data. Basic demographic or economic information is typically missing from these data sets, such as economic status of an individual, or their gender or age. If such information is available for a subset of the population in our database, an interesting challenge with broad policy implications is to use mobile phone data in predicting these demographic/economic/social indicators where such information is unavailable or not easily accessible. \n",
    "\n",
    "Thus, in supervised learning, one uses a given pair of input data and a corresponding supervisory target or desired target, $(y,X)$ to learn a function $f$ that can be used to predict the unknown target value of some input vector, i.e.\n",
    "$$ y = f(X).$$\n",
    "\n",
    "The learning process also uses some specified objective function to optimize when fitting to data. A simple is example of an objective function is the sum of squares error formula used in linear regression that you may be familiar with. The error term itself is usually referred to as the *[loss function](http://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing)* in machine learning, and performance of a given algorithm is commonly reported in terms of the loss function employed. Some of these will be discussed later in the notebook.\n",
    "\n",
    "Exactly what does learning entail? At its most basic, learning involves specifying a model structure $f$ that hopefully can extract regularities for the data or problem at hand as well as the appropriate objective function to optimize using a specified loss function. Learning (or fitting) the model essentially means finding optimal parameters of the model structure using provided input/target data. This is also called *training* the model. It is common (and best practice) to split the provided data into at least two sets - training and test data sets. The rationale for this split is the following. We are fitting the model to perform as well as it can using given or seen data. However, our goal is for the model to perform well on *unseen* data, that is data we still have to collect but whose labels ($y$) we do not know. To manage this challenge, we keep part of data hidden from the training or fitting process. Once satisfied we have an optimal model, we can then assess how well the model *generalizes* on unseen data using the data that was hidden from the training process. Data used to fit the model is referred to as training data whilst data kept hidden from the training but used to assess generalization performance of the model is called test data. There are different variations of this train/test split. You may also encounter the term validation data being used for essentially the same objective. (There are some technicalities that go beyond the scope of this course.)  \n",
    "\n",
    "\n",
    "Typically, one collects data of known $(y, X)$ pairs that hopefully captures the range of variability and associated outcomes for the domain problem on hand. Learning a function $f$ essentially involves capturing statistical regularities from systems that are potentially very complex and highly dimensional. Unlike classical scientific approaches, where the objective is finding a comprehensible law or theory to describe the nature or mechanics, in machine learning, the learned function $f$ is typically a complicated mathematical expression that describes the data e.g. the behavior of a economically disadvantaged person as observed through his telephone activity in a specific country.\n",
    "\n",
    "Python has a growing set of well developed toolkits that make it easier to perform this kind of data analysis, such as the [Scikit-Learn](http://scikit-learn.org/stable/) package commonly refered to as sklearn and [Mlxtend](http://rasbt.github.io/mlxtend/) (machine learning extensions), amongst others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 1 Start.</b>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Instructions\n",
    "Please provide brief written answers in markdown to demonstrate your understanding of the four concepts below:\n",
    " \n",
    " > 1. What is machine learning?\n",
    " \n",
    " > 2. How can machine learning be used?\n",
    " \n",
    " > 3. What is overfitting?\n",
    " \n",
    " > 4. How can you prevent overfitting?\n",
    "\n",
    "\n",
    "> **Hint**: Please review [this article](http://www.ma.utexas.edu/users/mks/statmistakes/ovefitting.html) as well as the [Wikipedia](https://en.wikipedia.org/wiki/Overfitting) page for additional information on overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your markdown answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 1 End.</b>\n",
    "</div>\n",
    "\n",
    "> **Exercise complete**:\n",
    "    \n",
    "> This is a good time to \"Save and Checkpoint\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Computing behavioral indicators\n",
    "## 2.1 Computing the indicators\n",
    "As discussed in the previous notebook, bandicoot allows us analyze and extract behavioral indicators from mobile phone data. With bandicoot, it is easy to load all the users in our Friends and Family dataset and automatically compute their indicators.\n",
    "\n",
    "The dataset provided contains 129 users interacting with each other. Each CSV file contains call and text metadata records belonging to a single user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load libraries and set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load libraries and set options.\n",
    "import matplotlib.pyplot as plt\n",
    "import bandicoot as bc\n",
    "from tqdm import tqdm_notebook as tqdm  # Interactive progress bar.\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 9) \n",
    "plt.rcParams['axes.titlesize'] = 'large'\n",
    "\n",
    "## For spectral graph partitioning.\n",
    "from sklearn.cluster import spectral_clustering as spc\n",
    "\n",
    "## Supervised learning.\n",
    "from sklearn import svm, linear_model, ensemble, neighbors, tree\n",
    "from sklearn import metrics, cross_validation, preprocessing\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from mlxtend.classifier import Perceptron\n",
    "from mlxtend.classifier import MultiLayerPerceptron as MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Create a function to load a user and returns all the indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_features(user_id):\n",
    "    '''\n",
    "    Compute and return indicators of a specific user\n",
    "    '''\n",
    "    user = bc.read_csv(user_id, \"bandicoot-training-master/data-fnf/records/\",\n",
    "                       attributes_path=\"../data/bandicoot/attributes/subsector/\",\n",
    "                       describe=False, warnings=False)\n",
    "\n",
    "    return bc.utils.all(user, summary='extended', split_day=True, split_week=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Create a list of users with their features from all the CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read all the csv files in the in \"./data-fnf/records\" directory.\n",
    "all_features = []\n",
    "\n",
    "# Process all the files in the specified directory using the function created previously.\n",
    "for f in glob.glob(\"./bandicoot-training-master/data-fnf/records/*.csv\"):\n",
    "    user_id = os.path.basename(f)[:-4]  # Remove .csv extension.\n",
    "    all_features.append(make_features(user_id))\n",
    "    \n",
    "# Export all features in one file (fnf_features.csv).\n",
    "bc.io.to_csv(all_features, 'fnf_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Load the features and attributes to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the features and attributes to a dataframe using the pandas library.\n",
    "df = pd.read_csv('fnf_features.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Defining the target variable\n",
    "In many situations, it is often that there’s a piece of important information that we are interested that is useful for informing policy initiatives or understanding human behaviour at scale. Particularly for the developing world, the ODI [noted](https://www.odi.org/sites/odi.org.uk/files/odi-assets/publications-opinion-files/9604.pdf):\n",
    "\n",
    ">\"Data are not just about measuring changes, they also facilitate and catalyse that change. Of course, good quality numbers will not change people’s lives in themselves. But to target the poorest systematically, to lift and keep them out of poverty, even the most willing governments cannot efficiently deliver services if they do not know who those people are, where they live and what they need. Nor do they know where their resources will have the greatest impact\"\n",
    "\n",
    "In the following example, we have provided an proxy indicator for socio-economic status that can be appended to the family and friends data set. (We could also use other attributes of interests such as age or gender as demonstrated in, for example, [A Study of Age and Gender seen through Mobile Phone Usage Patterns in Mexico](http://arxiv.org/abs/1511.06656)). Our objective is bulding a predictor of socio-economic segment one belongs to using mobile phone metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis (EDA) is a critical first step in any data analysis project and is useful for the following reasons.\n",
    "\n",
    "- Detecting errors in data\n",
    "- Validating our assumptions\n",
    "- Guide in the selection of appropriate models\n",
    "- Determining relationships among the explanatory variables\n",
    "- Assessing the direction and size (roughly) of relationships between predictor/explanatory and response/target variables\n",
    "\n",
    "We have already been using some basic EDA tools (```df.info()``` and ```df.describe()```) that an analyst may wish to use to have a glimpse on the data they will be working with. Other useful EDA approaches including preliminary screening of variables to assess how they relate with the response variable(s). In the following section, we demonstrate how one can explore differences in distributions of features between the groups we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First extract the groups by target variable.\n",
    "sub_gr=df.groupby('attributes__sub')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Are there any differences in the distribution of 'call durations' between the two groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot both segments.\n",
    "plt.subplot(2,2,1)\n",
    "_ = plt.hist(df['call_duration__allweek__allday__call__mean__mean'].dropna(), bins=25, color='green')\n",
    "plt.title('Both Segments')\n",
    "\n",
    "# Plot Segment 1.\n",
    "plt.subplot(2,2,2)\n",
    "_ = plt.hist(sub_gr.get_group(0) ['call_duration__allweek__allday__call__mean__mean'].dropna().values, bins=25, color='red') \n",
    "plt.title('Segment 1')\n",
    "\n",
    "# Plot Segment 2.\n",
    "plt.subplot(2,2,3)\n",
    "_ = plt.hist(sub_gr.get_group(1)['call_duration__allweek__allday__call__mean__mean'].dropna().values,bins=25)\n",
    "plt.title('Segment 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Are there any differences in the distribution of number of interactions between the two groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot both segments.\n",
    "plt.subplot(2,2,1)\n",
    "#df['call_duration__allweek__allday__call__mean__mean'].hist(bins=100)\n",
    "_ = plt.hist(df['number_of_interaction_in__allweek__night__text__mean'].dropna(), bins=25, color='green')\n",
    "plt.title('Both Segments')\n",
    "\n",
    "# Plot segment 1.\n",
    "plt.subplot(2,2,2)\n",
    "_ = plt.hist(sub_gr.get_group(0)['number_of_interaction_in__allweek__night__text__mean'].dropna().values, bins=25, color='red')\n",
    "plt.title('Segment 1')\n",
    "\n",
    "# Plot segment 2.\n",
    "plt.subplot(2,2,3)\n",
    "_ = plt.hist(sub_gr.get_group(1)['number_of_interaction_in__allweek__night__text__mean'].dropna().values,bins=25)\n",
    "plt.title('Segment 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the data to make it usable for machine learning purposes. This involves a number of activities such as \n",
    "1. Assigning numerical values to factors\n",
    "1. Handling missing values\n",
    "2. Normalize the features (so features on small scales do not dominate when fitting a model to the data)\n",
    "\n",
    "> **Note**: In order to achieve that and to have remaining data we impute the missing values. In other words, you make an educated guess about the missing data.\n",
    "\n",
    "Sklearn fuctions:\n",
    "- `preprocessing`\n",
    "- `preprocessing.Imputer()`\n",
    "\n",
    "> **Note**: Small scale imply big value. As example, assume we are classifying people based on their physiological characteristics. A small scale feature would be weight measured in kilograms vs height measured in meters. \n",
    "\n",
    "Sklearn functions:\n",
    "- `preprocessing.StandardScaler()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, make sure that we only have records in the dataset that are labeled. Our target variable is a socio-economic binary indicator   ```df.attributes__sub```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop records with missing labels in the target variable.\n",
    "df = df[~df.attributes__sub.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two objects to use in building the classifier:\n",
    "\n",
    "- the array ``y`` contains the labels we want to predict (cluster 1 / cluster 2),\n",
    "- the matrix ``X`` contains the features for all users (one column for one feature, one line for one user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Target variable.\n",
    "y = df.attributes__sub.astype(np.int)\n",
    "y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: The cell below will produce a warning message stating that \"A value is trying to be set on a copy of a slice from a DataFrame\" which you can ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert gender labels to binary values (zero or one):\n",
    "df.loc[:,'attributes__gender'] = (df.attributes__gender == 'female').values.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. Remove columns with reporting variables and attributes. (the first 39 and the last 2):\n",
    "df = pd.concat([df[df.columns[39:-5]],df[df.columns[-3]]], axis=1)\n",
    "X = df.values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Impute the missing values in the features.\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "Normalize the data to center it around zero and transform it to a similiar scale to prevent variables in 'small' units (and therefore high values) to dominate the classification unreasonably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. Preprocess data (center around 0 and scale to remove the variance).\n",
    "scaler = preprocessing.StandardScaler()\n",
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Classification with cross-validation\n",
    "\n",
    "> Support vector machines (SVMs) one of the popular linear classifiers that are based on two central ideas of margin maximization and kernel functions. Margin maximization allows one to find an optimal hyperplane that separates linearly data, while kernel functions extend the algorithm to handle data not linearly separable by mappping the data into a new high-dimensional space. A rough intuitive explanation of how it works can be found here: http://www.dataschool.io/comparing-supervised-learning-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen seen in the video and explained above, splitting the data into test and training sets is critical to avoid overfitting and, therefore, generalize to real previously unseen data. Cross-validation extends this idea further. Instead of having a single train/test split, we can specify so-called folds such that our data is divided into similarly sized folds. Training occurs by taking all folds except 1, also referred to as the holdout sample. On completion of training, we test the performance of our fitted model using the holdout sample. The holdout sample is then thrown back with the rest of the other folds, a different fold pulled out as the new holdout sample. Training is repeated again with the remaining folds and we measure performance using the holdout sample. This process is repeated until each fold had a chance to be a test or holdout sample. The expected performance of the classifier, called cross-validation error, is then simply an average of error rates computed on each holdout sample. We demonstrate this process first by performing a standard train/test split and then computing cross-validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5. Divide records in training and testing sets.\n",
    "np.random.seed(2)\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(Xs, y, test_size=0.3, stratify=y.values)\n",
    "\n",
    "# 6. Create an SVM classifier and train it on 70% of the data set.\n",
    "clf = svm.SVC(probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 7. Analyze accuracy of predictions on 30% of the holdout test sample.\n",
    "classifier_score = clf.score(X_test, y_test)\n",
    "print '\\nThe classifier accuracy score is {:.2f}\\n'.format(classifier_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model), we can successively split the data in folds that we use for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get average of 3-fold cross-validation score using an SVC estimator.\n",
    "n_folds = 3\n",
    "cv_error = np.average(cross_val_score(SVC(), Xs, y, cv=n_folds))\n",
    "print '\\nThe {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n'.format(n_folds, cv_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Receiver operating characteristic (ROC) curve. \n",
    "\n",
    "In statistical modeling and machine learning,  a commonly reported performance measure of model accuracy is Area Under the Curve (AUC), where by *curve* the ROC curve is implied. ROC stands for *Receiver Operating Characteristic* - a term originated from the second world war and used by by radar engineers.\n",
    "\n",
    "To understand what information the ROC curve conveys, consider the the so-called confusion matrix that basically is a 2-dimensional table where the classifier model is on one axis (vertical) and ground truth is on the other (horizontal) axis as shown below. Either of these axis can take two values as depicted. A cell in the table then is an intersection where the conditions on each the dimensions hold. For example, in the top left cell, the model condition is \"A\" and the ground truth is also \"A\". Hence, the count of instances where these 2 conditions are true (for a specific data point) is captured, hence the label 'True positive'. The same logic applies to the rest of the other cells. The total of the counts in these cells therefore must equal the number of data instances in our data set under consideration.\n",
    "\n",
    "\n",
    "~~~~\n",
    "                        Actual: A        Not A\n",
    "\n",
    "  Model says “A”       True positive   |   False positive\n",
    "                      ----------------------------------\n",
    "  Model says “Not A”   False negative  |    True negative\n",
    "  \n",
    "~~~~\n",
    "\n",
    "\n",
    "\n",
    "In an ROC curve, we plot ‘True Positive Rate‘ on Y-axis and ‘False Positive Rate‘ on the X-axis, where the the “true positive”, “false negative”, “false positive” and “true negative” are events (or their probability) as described above, and where the rates are defined according to:\n",
    "\n",
    "> True positive rate (or sensitivity)}:  **tpr = tp / (tp + fn)**\n",
    "\n",
    "> False positive rate:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                   **fpr = fp / (fp + tn)**\n",
    "\n",
    "> True negative rate (or specificity):   **tnr = tn / (fp + tn)**\n",
    "\n",
    "In all definitions the column total is the denominator. We can therefore express the true positive rate (tpr) as the probability that the model says “A” when the real value is indeed A (i.e., a conditional probability). This does not tell you how likely you are to be correct when calling “A” (i.e., the probability of a true positive, conditioned on the test result being “A”).\n",
    "\n",
    "To interpret the ROC correctly, consider the points that lie on along the diagonal represent. For these situation, there is an equal chance of \"A\" and \"not A\" happening. Therefore,  this is not that different from making a prediction by tossing of an unbiased coin, or put simply, the classification model is random.\n",
    "\n",
    "For points above the diagonal, **tpr** > **fpr**, and our model says we are in a zone where we are performing better than random. For example, assume **tpr ** = 0.6 and **fpr** = 0.2, then the probability of our being in the true positive group is $(0.6 / (0.6 + 0.2)) = 75\\%$. Furthermore, holding **fpr** constant, it is easy to see that the more vertically above the diagonal we are positioned, the better the classification model. Further basic details on the correct interpretation can be found in this [reference](\\http://pubs.rsna.org/doi/pdf/10.1148/radiographics.12.6.1439017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The confusion matrix helps visualize the performance of the algorithm.\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test.values, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the receiver operating characteristic curve (ROC).\n",
    "plt.figure(figsize=(20,10))\n",
    "probas_ = clf.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % (roc_auc))\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.axes().set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 2 Start.</b>\n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    " > What does the score and ROC curve tell us about our classifier and how it compares to \"flipping an unbiased coin\" (random) to determine the class to which an observed data point belongs?\n",
    " \n",
    " **Hint**\n",
    " > See more details regarding the ROC and its interpretation [here](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "\n",
    "Provide your answer in a markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 2 End.</b>\n",
    "</div>\n",
    "\n",
    "> **Exercise complete**:\n",
    "    \n",
    "> This is a good time to \"Save and Checkpoint\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 3 Start.</b>\n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    " > 1. How many data points in the test set from class 1 were predicted as class 1 by the trained classifier?\n",
    " > 2. How many data points in the test set from class 1 were predicted as class 0 by the trained classifier?\n",
    " > 3. How many data points in the test set from class 0 were predicted as class 0 by the trained classifier?\n",
    " > 4. How many data points in the test set from class 0 were predicted as class 1 by the trained classifier?\n",
    " > 5. What is the error rate on the test set for this classifier?\n",
    " > 6. Assuming class 0 as the positive class, can you calculate the true positive rate or sensitivity of this classifier?\n",
    " > 7. What is the specificity assuming class 1 is the negative class?\n",
    " \n",
    " **Hints**\n",
    " > Descriptions and formulae for **sensitivity** and **specificity** can be found [here](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your markdown answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 3 End.</b>\n",
    "</div>\n",
    "\n",
    "> **Exercise complete**:\n",
    "    \n",
    "> This is a good time to \"Save and Checkpoint\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimal classifier parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above made use of the SVC() function using default parameters. Usually, you would want to optimize the setting of these parameters for a given problem, as these are learned by the algorithm during the training phase. In the case of support vector classifiers, these parameters include kernel choice,\n",
    "the kernel parameters (Gaussian kernel: $\\gamma$; Polynomial kernel: $d$), as well as the penalty for misclassification ($C$). For an illustration of the behavior of these and other kernels explore the [scikit docs](http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html)resource. \n",
    "\n",
    "Tuning parameters for an optimal solution is inherently difficult. A popular approach is to perform a search over the a grid defined across the various parameters to be optimized for. The grid search function is illustrated next. This illustration will consider optimizing over two parameters - $C$ (misclassification cost) $\\gamma$ (The RBF kernel parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train classifiers.\n",
    "param_grid = {'C': np.logspace(-3, 2, 6), 'gamma': np.logspace(-3, 2, 6)}\n",
    "grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_.probability = True\n",
    "clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Using other classifiers\n",
    "\n",
    "The SVM classifier used is just one option of classifiers that you have at your disposal. There are other classification methods implemented in scikit-learn (as well as mlxtend) that you can easily use. These include:\n",
    "\n",
    "- Decision trees with ``tree.DecisionTreeClassifier()``;\n",
    "- K-nearest neighbors with ``neighbors.KNeighborsClassifier()``;\n",
    "- Random forests with ``ensemble.RandomForestClassifier()``;\n",
    "- Perceptron (both gradient and stochastic gradient) ``mlxtend.classifier.Perceptron``; and \n",
    "- Multilayer perceptron network (both gradient and stochastic gradient) ``mlxtend.classifier.MultiLayerPerceptron``.\n",
    "\n",
    "It is important to understand the underlying technique, as well as the implementation, in order to correctly interpret the output, or tune the estimator parameters. Next, the use of some of these classifiers is illustrated on the data set using the above-mentioned libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of random forest classifier, fit the data, and assess performance on test data.\n",
    "clf_rf = ensemble.RandomForestClassifier(n_estimators=200,n_jobs=-1,max_depth=5 )    \n",
    "n_folds = 3\n",
    "cv_error = np.average(cross_val_score(clf_rf, \n",
    "                                      X_train, \n",
    "                                      y_train, \n",
    "                                      cv=n_folds))\n",
    "clf_rf.fit(X_train, y_train)\n",
    "print '\\nThe {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n'.format(n_folds, cv_error)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of logistic regression classifier, fit the data, and assess performance on test data.\n",
    "clf_logreg = linear_model.LogisticRegression(C=1e5)    \n",
    "n_folds = 3\n",
    "cv_error = np.average(cross_val_score(clf_logreg, \n",
    "                                      X_train, \n",
    "                                      y_train, \n",
    "                                      cv=n_folds))\n",
    "clf_logreg.fit(X_train, y_train)\n",
    "print '\\nThe {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n'.format(n_folds, cv_error) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of decision tree classifier, fit the data, and assess performance on test data.\n",
    "clf_tree = tree.DecisionTreeClassifier()\n",
    "n_folds = 3\n",
    "cv_error = np.average(cross_val_score(clf_tree, \n",
    "                                      X_train, \n",
    "                                      y_train, \n",
    "                                      cv=n_folds))\n",
    "clf_tree = clf_tree.fit(X_train, y_train)\n",
    "print '\\nThe {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n'.format(n_folds, cv_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of multilayer perceptron classifier (gradient descent), fit the data, and assess performance on test data.\n",
    "clf_nn1 = MLP(hidden_layers=[40],l2=0.00,l1=0.0,epochs=150,eta=0.05,momentum=0.1,decrease_const=0.0,minibatches=1,random_seed=1,print_progress=3)\n",
    "clf_nn1 = clf_nn1.fit(X_train, y_train)\n",
    "clf_nn1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of multilayer perceptron classifier (stochastic gradient descent), fit the data, and assess performance on test data.\n",
    "clf_nn2 = MLP(hidden_layers=[40],l2=0.00,l1=0.0,epochs=50,eta=0.05,momentum=0.1,decrease_const=0.0,minibatches=len(y_train),random_seed=1,print_progress=3)\n",
    "clf_nn2 = clf_nn2.fit(X_train, y_train)\n",
    "clf_nn2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the results.\n",
    "colors = ['b', 'g', 'r','c','m','k','y']\n",
    "classifiers = ['svm','random_forest', 'logistic regression', 'decision tree', 'mlp_gd', 'mlp_sgd']\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, cl in enumerate([clf, clf_rf, clf_logreg, clf_tree,clf_nn1, clf_nn2]):\n",
    "    probas_ = cl.predict_proba(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=1, label=classifiers[i]+' (AUC = %0.2f)' % (roc_auc))\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], '--', color=colors[i], label='Random (AUC = 0.50)')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])   \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.axes().set_aspect(1)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 4 Start.</b>\n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    " > 1. Provide some general comments on the performance of the different classifiers based on the ROC plots.\n",
    " > 2. List two limitations you can think of that one should be aware of when using ROC curves to assess classifier performance.\n",
    " > 3. List two ways to improve the performance of the models.\n",
    "    \n",
    " **Hints**\n",
    " > As part of your general comments (requested in 1), rank the different classifiers based on the overall performance. Also, include comments on the relative performance of each classifier as the false positive rate is increased. Lastly, consider whether the use of an ensemble of these classifiers based on majority vote could improve the observed performance of the best classifier (general comments).\n",
    " \n",
    " >*Majority voting refers to using multiple classifiers for decision making, and returning as output the class which is in the majority. In other words, each classifier's output is considered a vote and the output that has the most votes is the winner.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your markdown answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 4 End.</b>\n",
    "</div>\n",
    "\n",
    "> **Exercise complete**:\n",
    "    \n",
    "> This is a good time to \"Save and Checkpoint\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 5 [Advanced exercise for bonus marks] Start.</b>\n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    " > 1. Build a $k$-nearest neighbor classifier using scikit-learn for $k=3$ and add it to the ROC plot as above. \n",
    " > 2. What effect does increasing $k$ have on the resulting classifier?\n",
    "    \n",
    " **Hints**\n",
    " > You will need to copy and paste the plot commands from above, and then modify the code by including the relevant code for a $k$-nearest neighbor classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 5 End.</b>\n",
    "</div>\n",
    "\n",
    "\n",
    "> **Exercise complete**:\n",
    "    \n",
    "> This is a good time to \"Save and Checkpoint\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Submit your notebook\n",
    "\n",
    "Please make sure that you:\n",
    "- Perform a final \"Save and Checkpoint\";\n",
    "- Download a copy of the notebook in \".ipynb\" format to your local machine using \"File\", \"Download as\", and \"IPython Notebook (.ipynb)\"; and\n",
    "- Submit a copy of this file to the online campus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. References\n",
    "Scikit-learn. 2010-2014. “1.4 Support Vector Machines.” Accessed August 16. http://scikit-learn.org/stable/modules/svm.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
