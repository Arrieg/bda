{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">Python 2.7 Jupyter Notebook</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your completion of the Notebook exercises will be graded based on your ability to: \n",
    "\n",
    "> **Apply**: Are you able to execute code, using the supplied examples, that perform the required functionality on supplied or generated data sets? \n",
    "\n",
    "> **Evaluate**: Are you able to interpret the results and justify your interpretation based on the observed data?\n",
    "\n",
    "> **Create**: Your ability to produce notebooks that serve as computational record of a session that can be used to share your insights with others? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook introduction\n",
    "\n",
    "Data collection is expensive and time consuming, as Arek Stopczynski alluded to in the video 2 resource on the learning path. \n",
    "In some cases you will be lucky enough to have existing datasets available to support your analysis. You may have datasets from previous analyses, access to providers, or curated datasets from your organization. In many cases, however, you will not have access to the data that you require to support your analysis, and you will have to find alternate mechanisms. \n",
    "The data quality requirements will differ based on the problem that you are trying to solve. Taking the hypothetical case of geocoding a location that was introduced in Module 1, the accuracy of the geocoded location does not need to be exact when you are simply trying to plot the locations of students on a map. Geocoding a location for an automated vehicle to turn off the highway, on the other hand, has an entirely different accuracy requirement.\n",
    "\n",
    "> **Note**:\n",
    "\n",
    "> Those of you who work in large organizations may be privileged enough to have company data governance and data quality initiatives. These efforts and teams can generally add significant value both in terms of supplying company-standard curated data, and making you aware of the internal policies that need to be adhered to.\n",
    "\n",
    "As a data analyst or data scientist, it is important to be aware of the implications of your decisions. You need to choose the appropriate set of tools and methods to deal with sourcing and supplying data.\n",
    "\n",
    "Technology has matured in recent years, and allowed access to a host of sources of data that can be used in our analyses. In many cases you can access free resources, or obtain data that has been curated, is at a lower latency, or comes with a service-level agreement at a cost. Some governments have even made datasets publicly available.\n",
    "\n",
    "You have been introduced to [OpenPDS](http://openpds.media.mit.edu/) in the video content where the focus shifts from supplying raw data - where the provider needs to apply security principles before sharing datasets - to supplying answers rather than data. OpenPDS allows users to collect, store, and control access to their data, while also allowing them to protect their privacy. In this way, users still have ownership of their data, as defined in the new deal on data. \n",
    "\n",
    "This notebook will demonstrate another example of sourcing external data to enrich your analyses. The Python ecosystem contains a rich set of tools and libraries that can help you to exploit the available resources.\n",
    "\n",
    "This course will not go into detail regarding the various options to source and interact with social data from sources such as Twitter, LinkedIn, Facebook, and Google Plus. However, you should be able to find libraries that will assist you in sourcing and manipulating these sources of data.\n",
    "\n",
    "Twitter data is a good example as, depending on the options selected by the twitter user, every tweet contains not just the message or content that most users are aware of. It also contains a view on the network of the person, home location, location from which the message was sent, and a number of other features that can be very useful when studying networks around a topic of interest. Professor Pentland pointed out the difference in what you share with the world (how you want to be seen) compared to what you actually do and believe (what you commit to). Ensure you keep these concepts in mind when you start exploring the additional sources of data. Those who are interested in the topic can start to explore the options by visiting the [twitter library on pypi](https://pypi.python.org/pypi/twitter). \n",
    "\n",
    "Start with the five Rs introduced in module 1, and consider the following questions:\n",
    "- How accurate does my dataset need to be?\n",
    "- How often should the dataset be updated?\n",
    "- What happens if the data provider is no longer available?\n",
    "- Do I need to adhere to any organizational standards to ensure consistent reporting or integration with other applications?\n",
    "- Are there any implications to getting the values wrong?\n",
    "\n",
    "You may need to start with “untrusted” data sources as a means of validating that your analysis can be executed. Once this is done, you can replace the untrusted components with trusted and curated datasets as your analysis matures.\n",
    "\n",
    "> **Note**: \n",
    "\n",
    "> It is strongly recommended that you save a checkpoint after applying significant changes or completing exercises. This allows you to return the notebook to a previous state should you wish to do so. On the Jupyter menu, select \"File\", then \"Save and Checkpoint\" from the dropdown menu that appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load libraries and set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_datareader import data, wb\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import folium\n",
    "import geocoder\n",
    "#import urllib2\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Source additional data from public sources \n",
    "## 1.1 World-bank\n",
    "\n",
    "This example will demonstrate how to source data from an external source to enrich your existing analyses. You will need to combine the data sources and add additional features to the example of student locations plotted on the world map in Module 1, Notebook 3.\n",
    "\n",
    "The specific indicator chosen has little relevance other than to demonstrate the process that you will typically follow in completing your projects. Population counts, from an untrusted source, is added to our map and we use scaling factors combined with the number of students and population size of the country to demonstrate adding external data with minimal effort.\n",
    "\n",
    "You can read more about the library that is utilized in this notebook [here](https://pandas-datareader.readthedocs.io/en/latest/remote_data.html#world-bank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the grouped_geocoded dataset from Module 1.\n",
    "df1 = pd.read_csv('data/grouped_geocoded.csv',index_col=[0])\n",
    "\n",
    "# Prepare the student location dataset for use in this example.\n",
    "# We use the geometrical center by obtaining the mean location for all observed coordinates per country.\n",
    "df2 = df1.groupby('country').agg({'student_count': [np.sum], 'lat': [np.mean], \n",
    "                                  'long': [np.mean]}).reset_index()\n",
    "# Reset the index.\n",
    "df3 = df2.reset_index(level=1, drop=True)\n",
    "\n",
    "# Get the external dataset from worldbank\n",
    "#  We have selected indicator, \"SP.POP.TOTL\"\n",
    "df4 = wb.download(\n",
    "                    # Specify indicator to retrieve\n",
    "                    indicator='SP.POP.TOTL',\n",
    "                    country=['all'],\n",
    "                    # Start Year\n",
    "                    start='2008',\n",
    "                    # End Year\n",
    "                    end=2016\n",
    "                )\n",
    "\n",
    "# The dataset contains entries for multiple years.\n",
    "#    We just want the last entry and create a separate object containing the list of maximum values\n",
    "df5 = df4.reset_index()\n",
    "idx = df5.groupby(['country'])['SP.POP.TOTL'].transform(max) == df4['SP.POP.TOTL']\n",
    "\n",
    "# Create a new dataframe where entries corresponds to maximum year indexes in previous list.\n",
    "df6 = df5[idx]\n",
    "\n",
    "# Combine the student and population datasets.\n",
    "df7 = pd.merge(df3, df6, on='country', how='left')\n",
    "\n",
    "# Rename the columns or our merged dataset.\n",
    "df8 = df7.rename(index=str, columns={('lat', 'mean'): \"lat_mean\", \n",
    "                                ('long', 'mean'): \"long_mean\", \n",
    "                                ('SP.POP.TOTL'): \"PopulationTotal_Latest_WB\",\n",
    "                                ('student_count', 'sum'): \"student_count\"}\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "\n",
    "> The visualization below does not have any meaning. The scaling factors selected is used to demonstrate the difference in population sizes and number of students on this course per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the combined dataset\n",
    "\n",
    "# Set map center and zoom level\n",
    "mapc = [0, 30]\n",
    "zoom = 2\n",
    "\n",
    "# Create map object.\n",
    "map_osm = folium.Map(location=mapc,\n",
    "                   tiles='Stamen Toner',\n",
    "                    zoom_start=zoom)\n",
    "\n",
    "# Plot each of the locations that we geocoded.\n",
    "for j in range(len(df8)):\n",
    "    # Plot a blue circle marker for country population.\n",
    "    folium.CircleMarker([df8.lat_mean[j], df8.long_mean[j]],\n",
    "                    radius=df8.PopulationTotal_Latest_WB[j]/500,\n",
    "                    popup='Population',\n",
    "                    color='#3186cc',\n",
    "                    fill_color='#3186cc',\n",
    "                   ).add_to(map_osm)\n",
    "    # Plot a red circle marker for students per country.\n",
    "    folium.CircleMarker([df8.lat_mean[j], df8.long_mean[j]],\n",
    "                    radius=df8.student_count[j]*10000,\n",
    "                    popup='Students',\n",
    "                    color='red',\n",
    "                    fill_color='red',\n",
    "                   ).add_to(map_osm)\n",
    "# Show the map.\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Copy the code from the previous two cells into the cells below. After you've reviewed the available indicators in the [worldbank](http://data.worldbank.org/indicator) dataset, replace the population indicator with an indicator of your choice. Add comments (lines starting with #) giving a brief description of your view on the observed results. Make sure to provide the tutor with a clear description of why you selected the indicator, what your expectation was when you started and what you think the results may indicate.\n",
    "\n",
    "> **Note**: Advanced users are welcome to source data from alternate data sources or manually upload files to be utilized to their virtual analysis environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your input to exercise 1.\n",
    "# \n",
    "# Add additional cells if you need them to show individual steps.\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Wikipedia\n",
    "\n",
    "### Exercise 2:\n",
    "\n",
    "To demonstrate how quickly data can be sourced from public, \"untrusted\" data sources, you have been supplied with a number of sample scripts below. While these sources contain extremely rich datasets that you can acquire with minimal effort, they can be amended by anyone and may not be 100% accurate. In some cases you will have to manually transform the datasets, while in others you might be able to use pre-built libraries.\n",
    "\n",
    "Execute the code cells below and think about the potential implications of using this type of data source when doing analysis or creating data products.\n",
    "\n",
    "**Please compile and submit a short list of pros and cons (three each). Your submission will be evaluated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install wikipedia\n",
    "import wikipedia\n",
    "\n",
    "# Display page summary\n",
    "print wikipedia.summary(\"MIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display a single sentence summary.\n",
    "wikipedia.summary(\"MIT\", sentences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create variable page that contains the wikipedia information.\n",
    "page = wikipedia.page(\"List of countries and dependencies by population\")\n",
    "\n",
    "# Display the page title.\n",
    "page.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the page URL. This can be utilised to create links back to descriptions.\n",
    "page.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here: You can provide feedback in a number of different formats.\n",
    "\n",
    "ListType\n",
    "- Element 1\n",
    "- Element 2\n",
    "\n",
    "| Type | Description |\n",
    "| ---- | ----------- |\n",
    "| Element 1 | Description 1 |\n",
    "| Element 2 | Description 2 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise complete**:\n",
    "    \n",
    "> This is a good time to \"Save and Checkpoint\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Submit your notebook\n",
    "\n",
    "Please make sure that you:\n",
    "- Perform a final \"Save and Checkpoint\";\n",
    "- Download a copy of the notebook in \".ipynb\" format to your local machine using \"File\", \"Download as\", and \"IPython Notebook (.ipynb)\"; and\n",
    "- Submit a copy of this file to the online campus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
