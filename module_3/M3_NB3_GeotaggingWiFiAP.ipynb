{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">Python 2.7 Jupyter Notebook</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geotagging WiFi Access points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your completion of the Notebook exercises will be graded based on your ability to: \n",
    "\n",
    "**All students**:\n",
    "\n",
    "> **Evaluate**: Are you able to interpret the results and justify your interpretation based on the observed data?\n",
    "\n",
    "\n",
    "**Advanced students**:\n",
    "\n",
    "> **Apply**: Are you able to execute code, using the supplied examples, that perform the required functionality on supplied or generated data sets? \n",
    "\n",
    "> **Analyze**: Are you able to pick the relevant method, library or resolve specific stated questions?\n",
    "\n",
    "> **Create**: Your ability to produce notebooks that serve as computational record of a session that can be used to share your insights with others? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook introduction\n",
    "\n",
    "This notebook will use the same Dartmouth 'Student Life' dataset as in previous exercises. In this exercise we will combine WiFi scans with location information to create a small database of WiFi AP location, using Google's location services. We will be replicating the work of Piotr Sapieżyński et al. [1].\n",
    "\n",
    "Let's start by importing the necessary modules and variable definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: \n",
    "\n",
    "> It is strongly recommended that you save a checkpoint after applying significant changes or completing exercises. This allows you to return the notebook to a previous state should you wish to do so. On the Jupyter menu, select \"File\", then \"Save and Checkpoint\" from the dropdown menu that appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load libraries and set options\n",
    "\n",
    "In order to compute the median and calculate the distances in section 1.4.2, you will need to import the custom function from the \"utils” directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load relevant libraries.\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# Load custom modules.\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import getmedian, haversine\n",
    "from utils import llaToECEF as coords_to_geomedian\n",
    "from utils import ECEFTolla as geomedian_to_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define variable definitions.\n",
    "wifi_path     = '../data/dartmouth/wifi'\n",
    "location_path = '../data/dartmouth/location/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single user review\n",
    "For simplicity you will start by reviewing a single user's data, examine properties of that data and try to see if analysis yields any results of value. You should be familiar with both WiFi and location data from previous exercises. As a result, they will be loaded and presented without extensive clarification.\n",
    "\n",
    "#### 1.1 Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load WiFi data.\n",
    "u00_wifi = pd.read_csv(path.join(wifi_path, 'wifi_u00.csv')) \n",
    "u00_wifi.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load location data.\n",
    "u00_loc = pd.read_csv(path.join(location_path, 'gps_u00.csv'))\n",
    "u00_loc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Remove columns that we do not require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove columns from WiFi dataset.\n",
    "u00_wifi.drop(['freq', 'level'], axis=1, inplace=True)\n",
    "u00_wifi.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove columns from location dataset.\n",
    "u00_loc.drop(['provider', 'network_type', 'bearing', 'speed', 'travelstate'], axis=1, inplace=True)\n",
    "u00_loc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Remove location records with insufficient accuracy\n",
    "The accuracy reported in location records is interpreted counterintuitively. The higher the value, the less accurate the measurement. It denotes the radius of a circle within which 68% of measurements, one standard deviation, of the reported coordinates is present. Since the radius of an outdoor access point can reach 250m [1], it is safe to assume a more conservative measure of 200m (at an elevated risk of classifying routers as non-stationary).\n",
    "\n",
    "The accuracy of location measurements is a major source of noise, hence the need for additional consideration. To do that we plot the cumulative distribution of the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot histogram of accuracy observations.\n",
    "u00_loc.accuracy.hist(cumulative=True, normed=1, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the dataset contains quite accurate location measurements. Visual inspection of the histogram suggests that almost 90% of observations have relatively good accuracy. It is therefore safe to select only the most accurate observations.\n",
    "\n",
    "Using the Pandas \"describe\" function, you can get a quick view of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Review the dataset with pandas decribe function.\n",
    "u00_loc.accuracy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, determine how many observations to keep. The impact of using an accuracy value of 40 is demonstrated in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine the number of records meeting our threshold of 40 for accuracy.\n",
    "result = len(u00_loc[u00_loc.accuracy <= 40]) / float(len(u00_loc))\n",
    "print 'Proportion of records that meet the criteria is {}%'.format(round(100*result,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73% of the records meet our criteria, and will be used as a filter in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a copy of the original dataset before applying the filter.\n",
    "u00_loc_raw = u00_loc.copy()\n",
    "\n",
    "# Apply the filter.\n",
    "u00_loc = u00_loc[u00_loc['accuracy'] <= 40]\n",
    "\n",
    "# Get the lenghts of each of the data objects.\n",
    "original_location_count = len(u00_loc_raw)\n",
    "filtered_location_count = len(u00_loc)\n",
    "\n",
    "print (\"Out of the original {0} location observations, {1} remain after filtering accuracy records \"\n",
    "       \"that does not meet our accuracy threshold.\".format(original_location_count, filtered_location_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the accuracy column from the dataset, as it is no longer required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update the object to remove accuracy.\n",
    "u00_loc.drop('accuracy', axis=1, inplace=True)\n",
    "\n",
    "# Display the head of the new dataset.\n",
    "u00_loc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "\n",
    "> In some cases you apply changes to datasets \"inplace\". While very efficient, you will no longer be able to re-execute earlier cells and this feature should be used with care. The guiding principle is that you can use it in data cleaning and wrangling steps, where you no longer need to go back and revisit earlier steps.\n",
    "\n",
    "> Should you need to revisit earlier steps, you can either restart the notebook and execute all the cells up to that point, or only the cells needed to get the object in the required form to continue your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Geotagging\n",
    "\n",
    "In order to geotag, location and Wi-Fi readouts need to be matched based on the time of the observations. As in the paper,  readouts will be constrained to those happening in exactly the same second to reduce impact of readouts from moving vehicles.\n",
    "\n",
    "There are three steps involved in geotagging:\n",
    "1. Match the records in time.\n",
    "2. Compute the median location of each AP.\n",
    "3. Filter out the nonstationary routers.\n",
    "\n",
    "These three steps will be explored in further detail in the following sections of this notebook.\n",
    "\n",
    "#### 1.4.1 Matching the records \n",
    "This requires the use of pandas magic to join (much like SQL join) the dataframes based on time. First, use the time as the index with the \"df.set_index()\" method, and then join them with the \"df.join()\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the index for WiFi.\n",
    "u00_wifi = u00_wifi.set_index('time')\n",
    "u00_wifi.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the index for location.\n",
    "u00_loc = u00_loc.set_index('time')\n",
    "u00_loc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having two dataframes with time as an index, we can simply \"join()\" them. \"df.join()\" by default performs a join (inner join in SQL) on the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join the two datasets and display the head of the new dataset.\n",
    "u00_raw_geotags = u00_wifi.join(u00_loc, how='inner')\n",
    "print '{} WiFi records found time matching location records.'.format(len(u00_raw_geotags))\n",
    "u00_raw_geotags.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to account for possible noise and remove the routers with sparse data (i.e. less than 5 observations, as in the paper). Pandas \"df.groupby()\" will be used to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create object u00_groups.\n",
    "u00_groups = u00_raw_geotags.groupby('BSSID')\n",
    "\n",
    "# Create a new object where filter criteria is met.\n",
    "u00_geotags = u00_groups.filter(lambda gr: len(gr)>=5)\n",
    "\n",
    "print (\"{} geotagged records remained after trimming for sparse data.\".format(len(u00_geotags)))\n",
    "print (\"They correspond to {} unique router APs\".format(len(u00_groups)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Compute the median location of each AP\n",
    "**Define stationary routers** as ones for which 95% of observations fall inside of 200 m from the geometric median of all the observations. In order to compute the median and calculate the distances you will need to import the custom function from the \"utils” directory.\n",
    "\n",
    "In order to compute the geometric medians with the tools at our disposal, the \"getmedian()\" methods need to be fed properly-formatted data. That means a lot of list points, where each point is an array of \"longitude\", \"latitude\", and \"altitude\". The algorithm accepts input in degrees as units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe with latitude and longitude.\n",
    "u00_geo_medians = pd.DataFrame(columns=[u'latitude', u'longitude'])\n",
    "\n",
    "# Transform the dataset using the provided set of utilities.\n",
    "for (BSSID, geotags) in u00_groups:\n",
    "    \n",
    "    geotags = [row for row in np.array(geotags[['latitude', 'longitude', 'altitude']])]\n",
    "    geotags = [coords_to_geomedian(row) for row in geotags]\n",
    "    \n",
    "    median  = getmedian(geotags)\n",
    "    median  = geomedian_to_coords(median)[:2]\n",
    "    \n",
    "    u00_geo_medians.loc[BSSID] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the above, you will have your geomedians, and be ready to move on to the last step, which is to filter out the non-stationary access points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the head of the geomedians object.\n",
    "u00_geo_medians.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Filter out the nonstationary routers\n",
    "**Identify stationary routers** with 95% confidence, and a distance threshold of 200m. Start by computing the distances using the \"haversine()\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the distances from the median.\n",
    "u00_distances = {}\n",
    "for BSSID, geotags in u00_groups:\n",
    "    u00_distances[BSSID] = []\n",
    "    (lat_median, lon_median) = u00_geo_medians.loc[BSSID]\n",
    "    for (lat, lon) in np.array(geotags[['latitude','longitude']]):  \n",
    "        u00_distances[BSSID].append(haversine(lon, lat, lon_median, lat_median)*1000) # haversine() returns distance in [km]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check how many of the routers pass the threshold. We iterate over the access points and count the ratio of measurements outside the threshold to all measurements. They are assigned to \"static\" or \"others\" based on our confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Group access points as static or non-static.\n",
    "# Set the thresholds\n",
    "distance_threshold = 200\n",
    "confidence_level    = 0.95\n",
    "\n",
    "# Create empty lists.\n",
    "static = []\n",
    "others = []\n",
    "\n",
    "for BSSID, distances in u00_distances.items():\n",
    "    \n",
    "    all_count  = len(distances)\n",
    "    near_count = len(filter(lambda distance: distance <= distance_threshold, distances))\n",
    "    \n",
    "    if( near_count / all_count >= confidence_level ):\n",
    "        static.append(BSSID)\n",
    "    else:\n",
    "        others.append(BSSID)\n",
    "\n",
    "# Print summary results.\n",
    "print (\"We identified {} static routers and {} non-static (moved or mobile).\".format(len(static), len(others))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tagged routers (access points) can now be visualized on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the access points on a map.\n",
    "map_center  = list(u00_geo_medians.median())\n",
    "routers_map = folium.Map(location=map_center, zoom_start=14)\n",
    "# Add points to the map for each of the locations.\n",
    "for router in static:\n",
    "      folium.CircleMarker(u00_geo_medians.loc[router], fill_color='red', radius=15, fill_opacity=0.5).add_to(routers_map)\n",
    "\n",
    "#Display the map.\n",
    "routers_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "\n",
    "> In order to validate your results, you can perform a \"rough\" check of validity by using the Google location API, which utilizes their own WiFi positioning. Set \"bssid1\" and \"bssid2\" to actual mac addresses of 2 routers located near each other to determine the location of a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import additional library and set access points to review.\n",
    "import requests\n",
    "bssid1 = '00:01:36:57:be:88'\n",
    "bssid2 = '00:01:36:57:be:87'\n",
    "\n",
    "# Use the Google API to return the geocoded location of the user location.\n",
    "url = 'https://maps.googleapis.com/maps/api/browserlocation/json?browser=' \\\n",
    "            'chrome&sensor=true&wifi=mac:{}&wifi=mac:{}'.format(bssid1, bssid2)\n",
    "\n",
    "# Save the response of the API call.\n",
    "response = requests.post(url)\n",
    "\n",
    "# Print the response.\n",
    "print response.json()['location']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compare this to your computed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u00_geo_medians.loc[[bssid1, bssid2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are acceptable. You can compute the actual distance between the points with the haversine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate and display the difference between calculated and Google API provided locations.\n",
    "lat_m1, lon_m1 = u00_geo_medians.loc[bssid1]\n",
    "lat_m2, lon_m2 = u00_geo_medians.loc[bssid2]\n",
    "\n",
    "lat, lon = response.json()['location'].values()\n",
    "\n",
    "print 'Distance from the Google API provided location to our first router ' \\\n",
    "            'estimation is {:2g}m'.format(haversine(lon,lat,lon_m1,lat_m1)*1000)\n",
    "print 'Distance from the Google API provided location to our first router ' \\\n",
    "            'estimation is {:2g}m'.format(haversine(lon,lat,lon_m2,lat_m2)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review of all users\n",
    "Next, we repeat the analysis from the previous section for all users. This analysis will be used in the next exercise.\n",
    "\n",
    "<br>\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Important.</b>\n",
    "Please ensure that this is the only running notebook when performing this section as you will require as much resources as possible to complete the next section. You have been introduced to shutting down notebooks in the refresh section, but can shut down running notebooks by viewing the \"Running\" tab on your Jupyter notebook directory view.\n",
    "</div>\n",
    "\n",
    "> **Note**:\n",
    "\n",
    "> We will provide less contextual information in this section as the details have already been provided in the previous section, section 1.\n",
    "\n",
    "#### 2.1 Load data for all users\n",
    "We utilize two new libraries that do not form part of the scope of this course in this section. Interested students can read more about [glob](https://docs.python.org/2.7/library/glob.html) to read files in the specified directory and [tqdm](https://github.com/tqdm/tqdm) used to render a progress bar. We start by setting our variables, then create the required function to process the input files and finally, execute this function to process all the files.\n",
    "\n",
    "> **Note**:\n",
    "\n",
    "> Processing large amounts of files or records can be time consuming and it is good practice to include progress bars to provide visual feedback where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set variables.\n",
    "all_geotags = pd.DataFrame(columns=['time','BSSID','latitude','longitude','altitude'])\n",
    "all_geotags = all_geotags.set_index('time')\n",
    "pcounter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define function to build the dataset, all_geotags, using the input files supplied.\n",
    "def build_ds(file_in, all_geotags):\n",
    "\n",
    "    # Get the user id.\n",
    "    user_id = path.basename(file_in)[5:-4]\n",
    "\n",
    "    # Read the WiFi and location data for the user.\n",
    "    wifi = pd.read_csv(file_in)\n",
    "    loc  = pd.read_csv(path.join(location_path, 'gps_'+user_id+'.csv'))\n",
    "\n",
    "    # Filter location data not meeting the accuracy threshold.\n",
    "    loc = loc[loc.accuracy <= 40]\n",
    "\n",
    "    # Drop the columns not required.\n",
    "    wifi.drop(['freq', 'level'], axis=1, inplace=True)\n",
    "    loc.drop(['accuracy', 'provider', 'network_type', 'bearing', 'speed', 'travelstate'], axis=1, inplace=True)\n",
    "\n",
    "    # Index the datasets based on time.\n",
    "    loc  = loc.set_index('time')\n",
    "    wifi = wifi.set_index('time')\n",
    "\n",
    "    # Join the datasets based on time index.\n",
    "    raw_tags = wifi.join(loc, how='inner')\n",
    "\n",
    "    # Return the dataset for the user.\n",
    "    return [raw_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iterate through the files in the specified directory and append the results of the function to the all_geotags variable.\n",
    "for f in tqdm(glob.glob(wifi_path + '/*.csv')):\n",
    "    # Append result from our function to all_geotags for each input file supplied.\n",
    "    all_geotags = all_geotags.append(build_ds(f, all_geotags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2) Drop APs with scarce records\n",
    "Remove access points with less than five observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"{} all geotags found\".format(len(all_geotags)))\n",
    "\n",
    "all_groups = all_geotags.groupby('BSSID')\n",
    "print (\"{} unique routers found\".format(len(all_groups)))\n",
    "\n",
    "# Drop sparsely populated access points.\n",
    "all_geotags = all_groups.filter(lambda gr: len(gr)>=5)\n",
    "all_groups = all_geotags.groupby('BSSID')\n",
    "\n",
    "print (\"{} unique router APs remaining after dropping routers with sparse data\".format(len(all_groups)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Compute medians\n",
    "Compute the medians for each router in the combined dataset as per section 1.4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new variable containing all the coordinates.\n",
    "all_geo_medians = pd.DataFrame(columns=[u'latitude', u'longitude'])\n",
    "\n",
    "# Compute the geomedians and add to all_geo_medians.\n",
    "\n",
    "# Initiate progress bar.\n",
    "with tqdm(total=len(all_groups)) as pbar:\n",
    "    # Iterate through data in all_groups as per single user example.\n",
    "    for i, data in enumerate(all_groups):\n",
    "        (BSSID, geotags) = data\n",
    "\n",
    "        geotags = [row for row in np.array(geotags[['latitude', 'longitude', 'altitude']])]\n",
    "        geotags = [coords_to_geomedian(row) for row in geotags]\n",
    "\n",
    "        median  = getmedian(geotags)\n",
    "        median  = geomedian_to_coords(median)[:2]\n",
    "\n",
    "        all_geo_medians.loc[BSSID] = median\n",
    "        pbar.update()\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Compute distances of observations to the calculated median\n",
    "Compute the distance from the medians for each router in the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the distances from the median.\n",
    "all_distances = {}\n",
    "\n",
    "# Initiate progress bar.\n",
    "with tqdm(total=len(all_groups)) as pbar:\n",
    "    # Iterate through data in all_groups as per single user example.\n",
    "    for i, data in enumerate(all_groups):\n",
    "        (BSSID, geotags) = data\n",
    "        all_distances[BSSID] = []\n",
    "        (lat_median, lon_median) = all_geo_medians.loc[BSSID]\n",
    "        for (lat, lon) in np.array(geotags[['latitude','longitude']]):  \n",
    "            all_distances[BSSID].append(haversine(lon, lat, lon_median, lat_median)*1000)\n",
    "        pbar.update()\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Label APs as static or non-static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Group access points as static or non-static.\n",
    "# Set the thresholds.\n",
    "distance_threshold = 200\n",
    "confidence_level   = 0.95\n",
    "\n",
    "# Create empty lists.\n",
    "all_static = []\n",
    "all_others = []\n",
    "\n",
    "for BSSID, distances in all_distances.items():\n",
    "    \n",
    "    all_count  = len(distances)\n",
    "    near_count = len(filter(lambda distance: distance <= distance_threshold, distances))\n",
    "    \n",
    "    if( near_count / all_count >= confidence_level ):\n",
    "        all_static.append(BSSID)\n",
    "    else:\n",
    "        all_others.append(BSSID)\n",
    "\n",
    "# Print summary results.\n",
    "print (\"We identified {} static routers and {} non-static (moved or mobile).\".format(len(all_static), len(all_others))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Plot the static APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the access points on a map.\n",
    "all_map_center  = list(all_geo_medians.median())\n",
    "all_routers_map = folium.Map(location=all_map_center, zoom_start=10)\n",
    "# Add points to the map for each of the locations.\n",
    "for router in all_static:\n",
    "      folium.CircleMarker(all_geo_medians.loc[router], fill_color='red', radius=15, fill_opacity=0.5).add_to(all_routers_map)\n",
    "\n",
    "#Display the map.\n",
    "all_routers_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 1 Start.</b>\n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    "\n",
    "> Review the visual output produced for all users. Remember that you can scroll and zoom on the map\n",
    "and try to find a static router located outside North America. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question**: Are you able to locate a static router located outside North America? If so, where? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Your markdown answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 1 End.</b>\n",
    "</div>\n",
    "> **Exercise complete**:\n",
    "    \n",
    "> This is a good time to \"Save and Checkpoint\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 2 [Advanced] Start.</b>\n",
    "</div>\n",
    "\n",
    "> **Note**:\n",
    "\n",
    "> This activity is for advanced users only and extra credit will be allocated. Standard users will not be penalized for not completing this activity.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "> Can you identify moving BSSIDs and plot the observed locations on a map?\n",
    "\n",
    "> This is not a trivial task compared to geotaging stationary routers, and there are many possible ways to perform the analysis.\n",
    "\n",
    "> Input : All datapoints for an Access Point \n",
    "\n",
    "> Output: Boolean mobility status  \n",
    "    \n",
    "> 1. Perform agglomerative clustering. (This is not covered in the scope of this course.)\n",
    "> 2. Discard clusters witn n<5 observations as noise.\n",
    "> 3. Compare pairwise timeframes in which each of the cluster was observed. \n",
    "> 4. If at least two clusters overlap (in time) consider the AP as mobile.\n",
    "    \n",
    "> **Note**:\n",
    "\n",
    "> Keep in mind that other, possibly better, solutions exist and we encourage you to provide us with your input.\n",
    "\n",
    "> **Hints**:\n",
    "\n",
    "> You can start by reviewing [Scipy](https://www.scipy.org/)\n",
    "\n",
    "> - `from scipy.spatial.distance import pdist`\n",
    "\n",
    "> - `from scipy.cluster.hierarchy import linkage, fcluster`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your answer here.\n",
    "#  Please add as many cells as you require in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your plot here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 2 End.</b>\n",
    "</div>\n",
    "> **Exercise complete**:\n",
    "    \n",
    "> This is a good time to \"Save and Checkpoint\".\n",
    "\n",
    "> We also recommend that you select \"Close and Halt\" when you have completed this notebook, to ensure that it does not continue to consume available resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Piotr Sapiezynski, Radu Gatej, Alan Mislove, Sune Lehmann, *Opportunities and Challenges in Crowdsourced Wardriving*, Proceedings of the 2015 ACM Conference on Internet Measurement Conference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Submit your notebook\n",
    "\n",
    "Please make sure that you:\n",
    "- Perform a final \"Save and Checkpoint\";\n",
    "- Download a copy of the notebook in \".ipynb\" format to your local machine using \"File\", \"Download as\", and \"IPython Notebook (.ipynb)\"; and\n",
    "- Submit a copy of this file to the online campus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
